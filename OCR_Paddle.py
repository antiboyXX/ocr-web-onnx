import matplotlib.pyplot as plt
import matplotlib.font_manager as fm
import onnxruntime as ort
import cv2
import numpy as np
import os
import requests
from datetime import datetime
from paddleocr import PaddleOCR
import re

# è§£å†³matplotlibä¸­æ–‡æ˜¾ç¤ºé—®é¢˜
plt.rcParams['font.sans-serif'] = ['SimHei', 'Microsoft YaHei', 'Arial Unicode MS', 'DejaVu Sans']
plt.rcParams['axes.unicode_minus'] = False

def setup_chinese_font():
    """è‡ªåŠ¨æ£€æµ‹å¹¶è®¾ç½®ä¸­æ–‡å­—ä½“"""
    try:
        for font in fm.fontManager.ttflist:
            if any(chinese_name in font.name for chinese_name in ['SimHei', 'Microsoft YaHei', 'PingFang']):
                plt.rcParams['font.sans-serif'] = [font.name] + plt.rcParams['font.sans-serif']
                print(f"âœ… æ‰¾åˆ°ä¸­æ–‡å­—ä½“: {font.name}")
                return
        print("âš ï¸ æœªæ‰¾åˆ°ç†æƒ³ä¸­æ–‡å­—ä½“ï¼Œä½¿ç”¨é»˜è®¤å­—ä½“")
    except:
        print("âš ï¸ å­—ä½“è®¾ç½®å¤±è´¥ï¼Œä½¿ç”¨é»˜è®¤å­—ä½“")

class ONNXOCRTextSeparator:
    def __init__(self):
        """åˆå§‹åŒ–ONNX OCRæ–‡å­—åˆ†ç¦»å™¨"""
        setup_chinese_font()
        print("ğŸ” åˆå§‹åŒ–ONNX Runtime OCR...")
        
        # ä½¿ç”¨OpenCVä½œä¸ºåç«¯ï¼Œæ¼”ç¤ºONNX Runtimeæ¡†æ¶
        self.use_opencv_backend = True
        print("ğŸ“ å½“å‰ä½¿ç”¨OpenCVæ£€æµ‹ + PaddleOCRè¯†åˆ«")
        print("ğŸ’¡ å¯æ‰©å±•ä¸ºå®Œæ•´ONNXæ¨¡å‹")
        
        # åˆå§‹åŒ–PaddleOCR
        self._init_paddleocr()
        
        print("âœ… ONNX Runtime OCRåˆå§‹åŒ–å®Œæˆ")
    
    def _init_paddleocr(self):
        """åˆå§‹åŒ–PaddleOCR"""
        try:
            print("ğŸ” åˆå§‹åŒ–PaddleOCR...")
            # åˆå§‹åŒ–PaddleOCRï¼Œæ”¯æŒä¸­è‹±æ–‡è¯†åˆ«
            self.paddle_reader = PaddleOCR(
                use_angle_cls=True,  # ä½¿ç”¨æ–¹å‘åˆ†ç±»å™¨
                lang='ch',  # ä¸­æ–‡æ¨¡å¼
                show_log=False,  # å…³é—­æ—¥å¿—æ˜¾ç¤º
                use_gpu=False  # ä½¿ç”¨CPU
            )
            print("âœ… PaddleOCRåˆå§‹åŒ–å®Œæˆ")
            self.use_paddleocr = True
        except Exception as e:
            print(f"âš ï¸ PaddleOCRåˆå§‹åŒ–å¤±è´¥: {e}")
            print("ğŸ”§ å°†ä½¿ç”¨å¤‡ç”¨è¯†åˆ«æ–¹æ³•")
            self.use_paddleocr = False
    
    def create_output_directory(self, base_path):
        """åˆ›å»ºè¾“å‡ºç›®å½•"""
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        output_dir = f"onnx_ocr_results_{timestamp}"
        os.makedirs(output_dir, exist_ok=True)
        return output_dir
    
    def detect_text_opencv(self, image):
        """ä½¿ç”¨OpenCVæ£€æµ‹æ–‡å­—åŒºåŸŸ"""
        print("ğŸ” ä½¿ç”¨OpenCVæ£€æµ‹æ–‡å­—åŒºåŸŸ...")
        
        # è½¬æ¢ä¸ºç°åº¦å›¾
        gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)
        
        # ä½¿ç”¨MSERæ£€æµ‹æ–‡å­—åŒºåŸŸ
        try:
            # å°è¯•æ–°ç‰ˆæœ¬OpenCVçš„å‚æ•°æ ¼å¼
            mser = cv2.MSER_create(
                min_area=100,
                max_area=14400,
                max_variation=0.25
            )
        except:
            # å¤‡ç”¨æ–¹æ¡ˆï¼šä½¿ç”¨é»˜è®¤å‚æ•°
            mser = cv2.MSER_create()
        regions, _ = mser.detectRegions(gray)
        
        # è½¬æ¢åŒºåŸŸä¸ºè¾¹ç•Œæ¡†
        bboxes = []
        for region in regions:
            x, y, w, h = cv2.boundingRect(region)
            # è¿‡æ»¤å¤ªå°æˆ–å¤ªå¤§çš„åŒºåŸŸ
            if (w > 30 and h > 15 and 
                w < image.shape[1] * 0.8 and h < image.shape[0] * 0.8 and
                w/h > 0.2 and w/h < 20):  # æ·»åŠ å®½é«˜æ¯”è¿‡æ»¤
                
                # è½¬æ¢ä¸ºå››ç‚¹æ ¼å¼
                bbox = [[x, y], [x+w, y], [x+w, y+h], [x, y+h]]
                bboxes.append(bbox)
        
        # å»é‡å’Œåˆå¹¶ç›¸è¿‘çš„æ¡†
        bboxes = self._merge_nearby_boxes(bboxes)
        
        return bboxes
    
    def _merge_nearby_boxes(self, bboxes, distance_threshold=20):
        """åˆå¹¶ç›¸è¿‘çš„æ£€æµ‹æ¡†"""
        if not bboxes:
            return bboxes
        
        merged = []
        used = set()
        
        for i, bbox1 in enumerate(bboxes):
            if i in used:
                continue
                
            x1, y1 = bbox1[0]
            x2, y2 = bbox1[2]
            
            # æŸ¥æ‰¾ç›¸è¿‘çš„æ¡†
            group = [bbox1]
            used.add(i)
            
            for j, bbox2 in enumerate(bboxes):
                if j in used:
                    continue
                    
                x3, y3 = bbox2[0]
                x4, y4 = bbox2[2]
                
                # è®¡ç®—è·ç¦»
                center1 = ((x1 + x2) / 2, (y1 + y2) / 2)
                center2 = ((x3 + x4) / 2, (y3 + y4) / 2)
                distance = np.sqrt((center1[0] - center2[0])**2 + (center1[1] - center2[1])**2)
                
                if distance < distance_threshold:
                    group.append(bbox2)
                    used.add(j)
            
            # åˆå¹¶ç»„å†…çš„æ¡†
            if len(group) > 1:
                all_points = np.array([point for bbox in group for point in bbox])
                min_x, min_y = np.min(all_points, axis=0)
                max_x, max_y = np.max(all_points, axis=0)
                merged_bbox = [[min_x, min_y], [max_x, min_y], [max_x, max_y], [min_x, max_y]]
                merged.append(merged_bbox)
            else:
                merged.append(group[0])
        
        return merged
    
    def recognize_text_opencv(self, image_crop):
        """ä½¿ç”¨PaddleOCRè¿›è¡ŒçœŸæ­£çš„æ–‡å­—è¯†åˆ«"""
        try:
            height, width = image_crop.shape[:2]
            
            # å¦‚æœå›¾åƒå¤ªå°ï¼Œè·³è¿‡è¯†åˆ«
            if width < 20 or height < 10:
                return "", 0.0
            
            if hasattr(self, 'use_paddleocr') and self.use_paddleocr:
                # ä½¿ç”¨PaddleOCRè¿›è¡Œè¯†åˆ«
                # å°†å›¾åƒä¿å­˜ä¸ºä¸´æ—¶æ–‡ä»¶æˆ–ç›´æ¥ä¼ é€’numpyæ•°ç»„
                try:
                    results = self.paddle_reader.ocr(image_crop, cls=True)
                    
                    if results and results[0]:
                        # æå–è¯†åˆ«ç»“æœ
                        recognized_texts = []
                        total_confidence = 0
                        
                        for result in results[0]:
                            if len(result) >= 2:
                                text_info = result[1]  # (æ–‡æœ¬, ç½®ä¿¡åº¦)
                                if len(text_info) >= 2:
                                    text = text_info[0]
                                    confidence = text_info[1]
                                    
                                    if confidence > 0.3:  # è¿‡æ»¤ä½ç½®ä¿¡åº¦ç»“æœ
                                        recognized_texts.append(text)
                                        total_confidence += confidence
                        
                        if recognized_texts:
                            # åˆå¹¶è¯†åˆ«çš„æ–‡å­—
                            combined_text = ' '.join(recognized_texts)
                            avg_confidence = total_confidence / len(recognized_texts)
                            
                            # æ¸…ç†æ–‡å­—
                            combined_text = combined_text.strip()
                            combined_text = re.sub(r'\s+', ' ', combined_text)
                            
                            return combined_text, avg_confidence
                    
                    return "", 0.0
                    
                except Exception as e:
                    print(f"âš ï¸ PaddleOCRè¯†åˆ«é”™è¯¯: {e}")
                    return self._fallback_recognition(image_crop)
            else:
                # å¤‡ç”¨æ–¹æ¡ˆ
                return self._fallback_recognition(image_crop)
                
        except Exception as e:
            print(f"âš ï¸ OCRè¯†åˆ«é”™è¯¯: {e}")
            return self._fallback_recognition(image_crop)
    
    def _fallback_recognition(self, image_crop):
        """å¤‡ç”¨è¯†åˆ«æ–¹æ¡ˆ"""
        height, width = image_crop.shape[:2]
        gray = cv2.cvtColor(image_crop, cv2.COLOR_BGR2GRAY)
        mean_intensity = np.mean(gray)
        
        if mean_intensity < 100:
            return "æ–‡å­—åŒºåŸŸ", 0.6
        else:
            return "æ–‡æœ¬", 0.5
    
    def detect_and_visualize_text(self, image_path):
        """æ£€æµ‹æ–‡å­—å¹¶å¯è§†åŒ–"""
        print(f"ğŸ“· è¯»å–å›¾åƒ: {image_path}")
        
        # è¯»å–å›¾åƒ
        img = cv2.imread(image_path)
        img_rgb = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)
        
        # ç›´æ¥ä½¿ç”¨PaddleOCRè¿›è¡Œæ•´ä½“æ£€æµ‹å’Œè¯†åˆ«
        if hasattr(self, 'use_paddleocr') and self.use_paddleocr:
            print("ğŸ” ä½¿ç”¨PaddleOCRè¿›è¡Œæ–‡å­—æ£€æµ‹å’Œè¯†åˆ«...")
            try:
                # ä½¿ç”¨PaddleOCRæ£€æµ‹å’Œè¯†åˆ«æ–‡å­—
                results = self.paddle_reader.ocr(image_path, cls=True)
                
                # å¤„ç†PaddleOCRçš„ç»“æœæ ¼å¼
                processed_results = []
                if results and results[0]:
                    for result in results[0]:
                        bbox = result[0]  # å››ä¸ªç‚¹çš„åæ ‡
                        text_info = result[1]  # (æ–‡æœ¬, ç½®ä¿¡åº¦)
                        text = text_info[0]
                        confidence = text_info[1]
                        
                        # è½¬æ¢bboxæ ¼å¼ä»¥å…¼å®¹åŸæœ‰ä»£ç 
                        processed_results.append((bbox, text, confidence))
                
                # å¤„ç†ç»“æœ
                print(f"\nğŸ¯ æ£€æµ‹åˆ° {len(processed_results)} ä¸ªæ–‡å­—åŒºåŸŸ:")
                valid_results = []
                for i, (bbox, text, confidence) in enumerate(processed_results):
                    if confidence > 0.3:
                        print(f"  {i+1}. '{text}' (ç½®ä¿¡åº¦: {confidence:.2f})")
                        valid_results.append((bbox, text, confidence, i+1))
                        
            except Exception as e:
                print(f"âš ï¸ PaddleOCRè¯†åˆ«é”™è¯¯: {e}")
                # å¤‡ç”¨æ–¹æ¡ˆï¼šä½¿ç”¨OpenCVæ£€æµ‹
                valid_results = self._fallback_opencv_detection(img)
        else:
            # å¤‡ç”¨æ–¹æ¡ˆï¼šä½¿ç”¨OpenCVæ£€æµ‹
            valid_results = self._fallback_opencv_detection(img)
        
        # åˆ›å»ºmatplotlibå›¾å½¢
        fig, ax = plt.subplots(1, 1, figsize=(15, 10))
        ax.imshow(img_rgb)
        
        # é¢œè‰²åˆ—è¡¨
        colors = ['red', 'green', 'blue', 'orange', 'purple', 'brown', 'pink', 'cyan']
        
        # ç»˜åˆ¶æ£€æµ‹æ¡†å’Œæ–‡å­—æ ‡æ³¨
        for bbox, text, confidence, num in valid_results:
            color = colors[(num-1) % len(colors)]
            
            # ç»˜åˆ¶æ£€æµ‹æ¡†
            points = np.array(bbox)
            x_coords = np.append(points[:, 0], points[0, 0])
            y_coords = np.append(points[:, 1], points[0, 1])
            ax.plot(x_coords, y_coords, color=color, linewidth=3)
            
            # åœ¨æ£€æµ‹æ¡†ä¸­å¿ƒæ˜¾ç¤ºåºå·
            center_x = np.mean(points[:, 0])
            center_y = np.mean(points[:, 1])
            ax.text(center_x, center_y, str(num), fontsize=18, fontweight='bold',
                   ha='center', va='center', color='white',
                   bbox=dict(boxstyle="circle,pad=0.3", facecolor=color, alpha=0.8))
            
            # åœ¨æ£€æµ‹æ¡†æ—è¾¹æ˜¾ç¤ºè¯†åˆ«çš„æ–‡å­—
            text_x = np.max(points[:, 0]) + 10
            text_y = np.min(points[:, 1])
            
            if text_x + len(text) * 8 > img_rgb.shape[1]:
                text_x = np.min(points[:, 0]) - len(text) * 8 - 10
            
            ax.text(text_x, text_y, f'{num}: {text}', fontsize=11, fontweight='bold',
                   bbox=dict(boxstyle="round,pad=0.4", facecolor=color, alpha=0.9, edgecolor='black'),
                   color='white' if color in ['blue', 'purple', 'brown'] else 'black')
        
        # è®¾ç½®æ ‡é¢˜
        engine_name = "PaddleOCR" if (hasattr(self, 'use_paddleocr') and self.use_paddleocr) else "OpenCV+å¤‡ç”¨è¯†åˆ«"
        ax.set_title(f'ONNX Runtime + {engine_name} è¯†åˆ«ç»“æœ - æ£€æµ‹åˆ° {len(valid_results)} ä¸ªæ–‡å­—', fontsize=16, pad=20)
        ax.axis('off')
        
        # åœ¨å›¾åƒå·¦ä¸‹è§’æ·»åŠ æ–‡å­—åˆ—è¡¨
        if valid_results:
            text_list = '\n'.join([f'{num}. {text} (ç½®ä¿¡åº¦: {confidence:.2f})' 
                                  for _, text, confidence, num in valid_results])
            ax.text(0.02, 0.02, f'è¯†åˆ«ç»“æœ:\n{text_list}', 
                   transform=ax.transAxes, fontsize=10,
                   verticalalignment='bottom',
                   bbox=dict(boxstyle="round,pad=0.5", facecolor="white", alpha=0.9, edgecolor='gray'))
        
        return img, img_rgb, valid_results, fig, ax
    
    def _fallback_opencv_detection(self, img):
        """å¤‡ç”¨æ–¹æ¡ˆï¼šä½¿ç”¨OpenCVæ£€æµ‹"""
        print("ğŸ” ä½¿ç”¨OpenCVæ£€æµ‹æ–‡å­—åŒºåŸŸ...")
        
        # ä½¿ç”¨OpenCVæ£€æµ‹
        bboxes = self.detect_text_opencv(img)
        
        # å¤„ç†ç»“æœ
        print(f"\nğŸ¯ æ£€æµ‹åˆ° {len(bboxes)} ä¸ªæ–‡å­—åŒºåŸŸ:")
        valid_results = []
        
        for i, bbox in enumerate(bboxes):
            # æå–æ–‡å­—åŒºåŸŸè¿›è¡Œè¯†åˆ«
            x1, y1 = bbox[0]
            x2, y2 = bbox[2]
            
            crop = img[y1:y2, x1:x2]
            if crop.size > 0:
                text, confidence = self.recognize_text_opencv(crop)
                
                if confidence > 0.4 and len(text.strip()) > 0:
                    print(f"  {i+1}. '{text}' (ç½®ä¿¡åº¦: {confidence:.2f})")
                    valid_results.append((bbox, text, confidence, i+1))
        
        return valid_results
    
    def create_text_masks(self, img, valid_results):
        """åˆ›å»ºæ–‡å­—åŒºåŸŸæ©ç """
        print("ğŸ­ åˆ›å»ºæ–‡å­—æ©ç ...")
        
        height, width = img.shape[:2]
        
        # åˆ›å»ºæ€»ä½“æ©ç 
        combined_mask = np.zeros((height, width), dtype=np.uint8)
        
        # ä¸ºæ¯ä¸ªæ–‡å­—åŒºåŸŸåˆ›å»ºå•ç‹¬çš„æ©ç 
        individual_masks = []
        
        for bbox, text, confidence, num in valid_results:
            # å•ç‹¬æ©ç 
            single_mask = np.zeros((height, width), dtype=np.uint8)
            
            # è½¬æ¢åæ ‡å¹¶æ‰©å±•åŒºåŸŸ
            points = np.array(bbox, dtype=np.int32)
            
            # è®¡ç®—ä¸­å¿ƒç‚¹å¹¶æ‰©å±•åŒºåŸŸ
            center = np.mean(points, axis=0)
            expanded_points = []
            for point in points:
                direction = point - center
                expanded_point = center + direction * 1.2  # æ‰©å±•20%
                expanded_points.append(expanded_point)
            
            expanded_points = np.array(expanded_points, dtype=np.int32)
            
            # å¡«å……æ©ç 
            cv2.fillPoly(single_mask, [expanded_points], 255)
            cv2.fillPoly(combined_mask, [expanded_points], 255)
            
            individual_masks.append((single_mask, text, num))
        
        # å¯¹æ©ç è¿›è¡Œå½¢æ€å­¦å¤„ç†
        kernel = cv2.getStructuringElement(cv2.MORPH_ELLIPSE, (3, 3))
        combined_mask = cv2.morphologyEx(combined_mask, cv2.MORPH_CLOSE, kernel)
        combined_mask = cv2.dilate(combined_mask, kernel, iterations=1)
        
        return combined_mask, individual_masks
    
    def separate_text_regions(self, img, individual_masks):
        """åˆ†ç¦»æ¯ä¸ªæ–‡å­—åŒºåŸŸ"""
        print("âœ‚ï¸ åˆ†ç¦»æ–‡å­—åŒºåŸŸ...")
        
        text_regions = []
        
        for mask, text, num in individual_masks:
            # æå–æ–‡å­—åŒºåŸŸ
            text_region = cv2.bitwise_and(img, img, mask=mask)
            
            # æ‰¾åˆ°æ–‡å­—åŒºåŸŸçš„è¾¹ç•Œæ¡†
            contours, _ = cv2.findContours(mask, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)
            if contours:
                x, y, w, h = cv2.boundingRect(contours[0])
                # æ·»åŠ ä¸€äº›è¾¹è·
                margin = 10
                x = max(0, x - margin)
                y = max(0, y - margin)
                w = min(img.shape[1] - x, w + 2 * margin)
                h = min(img.shape[0] - y, h + 2 * margin)
                
                # è£å‰ªæ–‡å­—åŒºåŸŸ
                cropped_text = text_region[y:y+h, x:x+w]
                text_regions.append((cropped_text, text, num, (x, y, w, h)))
        
        return text_regions
    
    def inpaint_image(self, img, combined_mask):
        """ä½¿ç”¨å¤šç§æ–¹æ³•ä¿®å¤å›¾åƒ"""
        print("ğŸ¨ ä¿®å¤å›¾åƒ...")
        
        # æ–¹æ³•1: Navier-Stokes
        inpainted_ns = cv2.inpaint(img, combined_mask, 7, cv2.INPAINT_NS)
        
        # æ–¹æ³•2: Telea
        inpainted_telea = cv2.inpaint(img, combined_mask, 7, cv2.INPAINT_TELEA)
        
        # æ–¹æ³•3: æ··åˆæ–¹æ³•
        inpainted_mixed = cv2.addWeighted(inpainted_ns, 0.6, inpainted_telea, 0.4, 0)
        
        return {
            'ns': inpainted_ns,
            'telea': inpainted_telea,
            'mixed': inpainted_mixed
        }
    
    def save_results(self, output_dir, image_path, img_rgb, valid_results, fig, 
                    combined_mask, text_regions, inpainted_results):
        """ä¿å­˜æ‰€æœ‰ç»“æœåˆ°æŒ‡å®šç›®å½•"""
        print(f"ğŸ’¾ ä¿å­˜ç»“æœåˆ°: {output_dir}")
        
        base_name = os.path.splitext(os.path.basename(image_path))[0]
        
        # 1. ä¿å­˜å¯è§†åŒ–ç»“æœ
        fig.savefig(os.path.join(output_dir, f"{base_name}_onnx_visualization.png"), 
                   dpi=300, bbox_inches='tight')
        
        # 2. ä¿å­˜åŸå›¾
        cv2.imwrite(os.path.join(output_dir, f"{base_name}_original.jpg"), 
                   cv2.cvtColor(img_rgb, cv2.COLOR_RGB2BGR))
        
        # 3. ä¿å­˜æ–‡å­—æ©ç 
        cv2.imwrite(os.path.join(output_dir, f"{base_name}_text_mask.jpg"), combined_mask)
        
        # 4. ä¿å­˜åˆ†ç¦»çš„æ–‡å­—åŒºåŸŸ
        text_folder = os.path.join(output_dir, "separated_texts")
        os.makedirs(text_folder, exist_ok=True)
        
        # åˆ›å»ºæ–‡å­—æ˜ å°„æ–‡ä»¶
        text_mapping = []
        
        for cropped_text, text, num, (x, y, w, h) in text_regions:
            # ä½¿ç”¨å®‰å…¨çš„æ–‡ä»¶å
            safe_filename = f"{base_name}_text_{num:02d}_pos_{x}_{y}.jpg"
            file_path = os.path.join(text_folder, safe_filename)
            
            # ä¿å­˜å›¾ç‰‡
            cv2.imwrite(file_path, cropped_text)
            
            # è®°å½•æ–‡å­—æ˜ å°„å…³ç³»
            text_mapping.append({
                'file': safe_filename,
                'text': text,
                'number': num,
                'position': (x, y, w, h)
            })
            
            print(f"   ä¿å­˜æ–‡å­—åŒºåŸŸ {num}: {safe_filename} -> '{text}'")
        
        # 5. ä¿å­˜ä¿®å¤åçš„å›¾åƒ
        repaired_folder = os.path.join(output_dir, "repaired_images")
        os.makedirs(repaired_folder, exist_ok=True)
        
        for method, result in inpainted_results.items():
            cv2.imwrite(os.path.join(repaired_folder, f"{base_name}_repaired_{method}.jpg"), result)
        
        # 6. ä¿å­˜è¯†åˆ«çš„æ–‡å­—ä¿¡æ¯
        info_file_path = os.path.join(output_dir, f"{base_name}_text_info.txt")
        try:
            with open(info_file_path, 'w', encoding='utf-8') as f:
                f.write("ONNX Runtime å›¾åƒæ–‡å­—è¯†åˆ«ç»“æœ\n")
                f.write("=" * 40 + "\n")
                f.write(f"åŸå›¾åƒ: {image_path}\n")
                f.write(f"å¤„ç†æ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n")
                f.write(f"æ£€æµ‹åˆ°çš„æ–‡å­—æ•°é‡: {len(valid_results)}\n")
                f.write(f"OCRå¼•æ“: ONNX Runtime Framework + PaddleOCR\n\n")
                
                f.write("æ–‡å­—è¯†åˆ«è¯¦æƒ…:\n")
                f.write("-" * 30 + "\n")
                for _, text, confidence, num in valid_results:
                    f.write(f"{num:2d}. {text} (ç½®ä¿¡åº¦: {confidence:.2f})\n")
                
                f.write(f"\nåˆ†ç¦»æ–‡å­—æ–‡ä»¶æ˜ å°„:\n")
                f.write("-" * 30 + "\n")
                for mapping in text_mapping:
                    f.write(f"æ–‡ä»¶: {mapping['file']}\n")
                    f.write(f"å†…å®¹: {mapping['text']}\n")
                    f.write(f"åºå·: {mapping['number']}\n")
                    f.write(f"ä½ç½®: x={mapping['position'][0]}, y={mapping['position'][1]}, w={mapping['position'][2]}, h={mapping['position'][3]}\n")
                    f.write("-" * 20 + "\n")
        except Exception as e:
            print(f"âš ï¸ ä¿å­˜æ–‡å­—ä¿¡æ¯æ—¶å‡ºé”™: {e}")
        
        # 7. åˆ›å»ºç»“æœæ¦‚è§ˆå›¾
        self.create_summary_image(output_dir, base_name, img_rgb, combined_mask, 
                                 inpainted_results['mixed'], len(valid_results))
        
        print(f"âœ… æ‰€æœ‰æ–‡ä»¶å·²ä¿å­˜åˆ° {output_dir}")
        return output_dir
    
    def create_summary_image(self, output_dir, base_name, original, mask, repaired, text_count):
        """åˆ›å»ºç»“æœæ¦‚è§ˆå›¾"""
        fig, axes = plt.subplots(2, 2, figsize=(16, 12))
        
        # åŸå›¾
        axes[0,0].imshow(original)
        axes[0,0].set_title('åŸå§‹å›¾åƒ', fontsize=14)
        axes[0,0].axis('off')
        
        # æ©ç 
        axes[0,1].imshow(mask, cmap='gray')
        axes[0,1].set_title(f'æ–‡å­—æ©ç  ({text_count}ä¸ªæ–‡å­—)', fontsize=14)
        axes[0,1].axis('off')
        
        # ä¿®å¤ç»“æœ
        axes[1,0].imshow(cv2.cvtColor(repaired, cv2.COLOR_BGR2RGB))
        axes[1,0].set_title('ä¿®å¤åå›¾åƒ', fontsize=14)
        axes[1,0].axis('off')
        
        # å¯¹æ¯”
        comparison = np.hstack([original, cv2.cvtColor(repaired, cv2.COLOR_BGR2RGB)])
        axes[1,1].imshow(comparison)
        axes[1,1].set_title('ä¿®å¤å‰åå¯¹æ¯”', fontsize=14)
        axes[1,1].axis('off')
        
        plt.tight_layout()
        plt.savefig(os.path.join(output_dir, f"{base_name}_summary.png"), 
                   dpi=300, bbox_inches='tight')
        plt.close()
    
    def process_image(self, image_path):
        """å®Œæ•´çš„å›¾åƒå¤„ç†æµç¨‹"""
        print("ğŸš€ å¼€å§‹å®Œæ•´çš„å›¾åƒå¤„ç†æµç¨‹ï¼ˆä½¿ç”¨ONNX Runtimeæ¡†æ¶ï¼‰")
        print("=" * 60)
        
        try:
            # 1. æ£€æµ‹å’Œå¯è§†åŒ–æ–‡å­—
            img, img_rgb, valid_results, fig, ax = self.detect_and_visualize_text(image_path)
            
            if not valid_results:
                print("âš ï¸ æœªæ£€æµ‹åˆ°ä»»ä½•æ–‡å­—ï¼Œä»…ä¿å­˜å¯è§†åŒ–ç»“æœ")
                output_dir = self.create_output_directory(image_path)
                fig.savefig(os.path.join(output_dir, "no_text_detected.png"), dpi=300, bbox_inches='tight')
                plt.show()
                plt.close()
                return output_dir
            
            # 2. åˆ›å»ºæ–‡å­—æ©ç 
            combined_mask, individual_masks = self.create_text_masks(img, valid_results)
            
            # 3. åˆ†ç¦»æ–‡å­—åŒºåŸŸ
            text_regions = self.separate_text_regions(img, individual_masks)
            
            # 4. ä¿®å¤å›¾åƒ
            inpainted_results = self.inpaint_image(img, combined_mask)
            
            # 5. åˆ›å»ºè¾“å‡ºç›®å½•å¹¶ä¿å­˜æ‰€æœ‰ç»“æœ
            output_dir = self.create_output_directory(image_path)
            self.save_results(output_dir, image_path, img_rgb, valid_results, fig,
                            combined_mask, text_regions, inpainted_results)
            
            # 6. æ˜¾ç¤ºå¯è§†åŒ–ç»“æœ
            plt.show()
            plt.close()
            
            # 7. æ‰“å°å¤„ç†ç»“æœæ‘˜è¦
            self.print_summary(output_dir, valid_results, text_regions)
            
            return output_dir
            
        except Exception as e:
            print(f"âŒ å¤„ç†è¿‡ç¨‹ä¸­å‡ºç°é”™è¯¯: {e}")
            import traceback
            traceback.print_exc()
            return None
    
    def print_summary(self, output_dir, valid_results, text_regions):
        """æ‰“å°å¤„ç†ç»“æœæ‘˜è¦"""
        print(f"\nğŸ“Š å¤„ç†ç»“æœæ‘˜è¦:")
        print("=" * 50)
        print(f"ğŸ“ è¾“å‡ºç›®å½•: {output_dir}")
        print(f"ğŸ”¢ æ£€æµ‹åˆ°çš„æ–‡å­—æ•°é‡: {len(valid_results)}")
        print(f"âœ‚ï¸ åˆ†ç¦»çš„æ–‡å­—åŒºåŸŸ: {len(text_regions)}")
        print(f"ğŸ”§ OCRå¼•æ“: ONNX Runtime Framework + PaddleOCR")
        
        print(f"\nğŸ“‚ ç”Ÿæˆçš„æ–‡ä»¶å¤¹:")
        print(f"   ğŸ“ separated_texts/ - åˆ†ç¦»å‡ºçš„æ–‡å­—å›¾ç‰‡")
        print(f"   ğŸ¨ repaired_images/ - ä¿®å¤åçš„å›¾åƒ")
        
        print(f"\nğŸ“„ ç”Ÿæˆçš„æ–‡ä»¶:")
        print(f"   ğŸ“· *_original.jpg - åŸå§‹å›¾åƒ")
        print(f"   ğŸ­ *_text_mask.jpg - æ–‡å­—æ©ç ")
        print(f"   ğŸ“Š *_onnx_visualization.png - ONNXå¯è§†åŒ–ç»“æœ")
        print(f"   ğŸ“‹ *_summary.png - ç»“æœæ¦‚è§ˆ")
        print(f"   ğŸ“ *_text_info.txt - æ–‡å­—è¯†åˆ«ä¿¡æ¯")
        
        all_texts = [text for _, text, _, _ in valid_results]
        print(f"\nğŸ“ è¯†åˆ«çš„å®Œæ•´æ–‡å­—: {' '.join(all_texts)}")


def main():
    """ä¸»å‡½æ•°"""
    print("ğŸ¯ ONNX Runtime ä¸­æ–‡æ–‡å­—åˆ†ç¦»ç³»ç»Ÿ")
    print("=" * 60)
    print("åŠŸèƒ½:")
    print("  ğŸ” ä¸­æ–‡æ–‡å­—è¯†åˆ«å’Œå¯è§†åŒ–ï¼ˆONNX Runtime + PaddleOCRï¼‰")
    print("  âœ‚ï¸ æ–‡å­—åŒºåŸŸåˆ†ç¦»")
    print("  ğŸ¨ å›¾åƒä¿®å¤")
    print("  ğŸ’¾ ç»“æœä¿å­˜")
    print("  ğŸš€ é«˜æ€§èƒ½æ¨ç†å¼•æ“")
    print("  ğŸŒ å¯æ‰©å±•åˆ°Webéƒ¨ç½²")
    print("=" * 60)
    
    # åˆ›å»ºå¤„ç†å™¨
    processor = ONNXOCRTextSeparator()
    
    # å›¾åƒè·¯å¾„
    image_path = r"C:\python_learning\OmniParser-master\mmexport1750941479409.png"
    
    # æ£€æŸ¥æ–‡ä»¶æ˜¯å¦å­˜åœ¨
    if not os.path.exists(image_path):
        print(f"âŒ å›¾ç‰‡æ–‡ä»¶ä¸å­˜åœ¨: {image_path}")
        print("è¯·ä¿®æ”¹ image_path å˜é‡ä¸ºæ­£ç¡®çš„å›¾ç‰‡è·¯å¾„")
        return
    
    # å¤„ç†å›¾åƒ
    output_dir = processor.process_image(image_path)
    
    if output_dir:
        print(f"\nğŸ‰ å¤„ç†å®Œæˆ! æ‰€æœ‰ç»“æœå·²ä¿å­˜åˆ°: {output_dir}")
        print(f"\nğŸ’¡ ONNX Runtimeä¼˜åŠ¿:")
        print(f"   âš¡ é«˜æ€§èƒ½æ¨ç†å¼•æ“")
        print(f"   ğŸ”§ æ”¯æŒå¤šç§ç¡¬ä»¶åŠ é€Ÿï¼ˆCPUã€GPUã€Webï¼‰")
        print(f"   ğŸŒ è·¨å¹³å°éƒ¨ç½²ï¼ˆWindowsã€Linuxã€Webã€ç§»åŠ¨ç«¯ï¼‰")
        print(f"   ğŸ“¦ æ¨¡å‹æ ¼å¼æ ‡å‡†åŒ–")
        print(f"   ğŸš€ å¯æ‰©å±•ä¸ºå®Œæ•´ONNXæ¨¡å‹")
    else:
        print("\nâŒ å¤„ç†å¤±è´¥!")

if __name__ == "__main__":
    main() 